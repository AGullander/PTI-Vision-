# OpenVINO GenAI Configuration (local inference - no cloud API needed!)
USE_OPENVINO=1

# Path to converted OpenVINO LLM model for text reasoning
OPENVINO_LLM_PATH=./models/TinyLlama_ov

# Path to converted OpenVINO VLM model for vision analysis
OPENVINO_VLM_PATH=./models/minicpm_ov

# Device for OpenVINO inference: CPU, GPU, or NPU
OPENVINO_DEVICE=CPU

# Disable OpenAI (using local OpenVINO instead)
USE_OPENAI_VISION=0
USE_OPENAI_TEXT=0

# API Key for your backend (keep for security)
API_KEY=pti_demo_key_2025
